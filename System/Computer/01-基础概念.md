<!-- TOC -->

- [冯诺依曼体系](#冯诺依曼体系)
- [计算单位](#计算单位)
    - [容量单位](#容量单位)
    - [速度单位](#速度单位)
        - [网络速度](#网络速度)
        - [CPU 频率](#cpu-频率)
- [数与进制](#数与进制)
    - [原码表示法](#原码表示法)
    - [补码表示法](#补码表示法)
        - [逻辑右移和算术右移](#逻辑右移和算术右移)
    - [浮点数](#浮点数)
- [字符与编码集](#字符与编码集)
    - [ASCII](#ascii)
    - [Extended ASCII](#extended-ascii)
    - [Unicode](#unicode)
        - [UTF-8](#utf-8)
        - [UTF-16](#utf-16)
    - [中文编码集](#中文编码集)
- [地址与字节对齐](#地址与字节对齐)
- [大端与小端](#大端与小端)
- [资源](#资源)

<!-- /TOC -->

# 冯诺依曼体系

将**程序指令和数据一起存储**的计算机设计概念结构.

早起计算机仅包含固定用途程序, 如果说要改变程序得更改结构或者重新设计电路, 这非常不方便且耗钱.

- 必须有一个存储器
- 必须有一个控制器
- 必须有输入设备
- 必须有输出设备

现代计算机都是冯诺依曼计算机


# 计算单位

## 容量单位

- 在物理层面, 高低电平记录信息
- 理论上只认识 0/1 两种状态(0/1 称为 bit)
- 0/1 能够表示的内容太少, 需要更大的容量表示方法
- 字节: 1Byte=8bits

` `|bit|Byte|KB|MB|GB|TB|PB|EB
:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:
名字|比特位|字节|千字节|兆字节|吉字节|太字节|拍字节|艾字节
比例|-|8bits|1024B|1024KB|1024MB|1024GB|1024TB|1024EB
常见设备|门电路|-|寄存器|高速缓存|内存/硬盘|硬盘|云硬盘|数据仓库

- 硬盘商使用 10 进制标记容量

## 速度单位

### 网络速度

- 网络常用的单位为 Mbps<br>
    100M/s = 100Mbps = 100Mbit/s = (100/8)MB/s = 12.5MB/s

### CPU 频率

- CPU 的速度一般体现为 CPU 的时钟频率
- CPU 的时钟频率的单位一般是赫兹(Hz)
- 主流 CPU 的时钟频率都在 2GHz 以上
- Hz 表示秒分之一<br>
    它是每秒中的周期性变动重复次数的计量<br>
    并不是描述计算机领域所专有的单位<br>

# 数与进制

- 进位制是一种记数方式, 亦称进位计数法或位值计数法
- 有限种数字符号来表示无限的数值
- 使用的数字符号的数目称为这种进位制的基数或底数

> 在计算机术语中，把二进制数中的某一位数又称为一个比特（bit）。比特这个单位对于计算机而言，在度量上是最小的单位。除了比特之外，还有字节（byte）这个术语。一个字节由 8 个比特构成。在某些单片机架构下还引入了半字节（nybble 或 nibble）这个概念，表示 4 个比特。然后，还有字（word）这个术语。字在不同计算机架构下表示的含义不同。在 x86 架构下，一个字为 2 个字节；而在 ARM 等众多 32 位 RISC 体系结构下，一个字表示为 4 个字节。随着计算机带宽的提升，能被处理器一次处理的数据宽度也不断提升，因此出现了双字（double word）、四字（quad word）、八字（octa word）等概念。双字的宽度为 2 个字，四字宽度为 4 个字，所以它们在不同处理器体系结构下所占用的字节个数也会不同。

## 原码表示法

> 而对于含有正负符号的原码，其二进制表示含有一位符号位，用于表示正负号。一般都是以二进制数的最高有效位（即最左边的比特）作为符号位，其余各位比特表示该数的绝对值大小。比如，十进制数 6 用一个 8 位的原码表示为 00000110；如果是 -6，则表示为 10000110。<br>
> 原码的表示非常直观，但是对于计算机算术运算而言就带来了许多麻烦。比如，我们用上述的 6 与 -6 相加，即 00000110+10000110，结果为 10001100，也就是十进制数 -12，显然不是我们想要的结果。所以，如果某个处理器用原码表示二进制数，那么它参与加减法的时候必须对两个操作数的正负符号加以判断，然后再判定使用加法操作还是减法操作，最后还要判定结果的正负符号，可谓相当麻烦。所以，当前计算机的处理器往往采用补码的方式来表达带符号的二进制数。

## 补码表示法

> 正由于原码含有上述缺点，所以人们开发出了另一种带符号的二进制码表示法——补码。补码与原码一样，用最高位比特表示符号位，其余各位比特则表示数值大小。如果符号位为 0，说明整个二进制数为正数或零；如果为 1，那么表示整个二进制数为负数。当符号位为0时，二进制补码表示法与原码一模一样，但是当符号位为负数时，情况就完全不同了。此时，对二进制数的补码表示需要按以下步骤进行：
> 1. 先将该二进制数以绝对值的原码形式写好；
> 2. 对整个二进制数（包括符号位），每一个比特都取反。所谓取反就是说，原来一个比特的数值为 0 时，则要变 1；为 1 时，则要变 0。
> 3. 变换好之后，将二进制数做加 1 计算，最终结果就是该负数的补码值了。<br>

```sh
# -6 的原码表示
  -6 | 10000110
# -6 的绝对值原码
   6 | 00000110
# -6 绝对值取反
     | 11111001
# -6 的补码
     | 11111010
```

### 逻辑右移和算术右移

当二进制数的值表示图形模式而非数值时，移位后需要在最高位补 0。类似于霓虹灯往右滚动的效果。这就称为逻辑右移。

右移有移位后在最高位补 0 和补 1 两种情况。将二进制数作为带符号的数值进行运算时，移位后要在最高位填充移位前符号位的值（0 或 1）。这就称为算术右移。如果数值是用补数表示的负数值，那么右移后在空出来的最高位补 1，就可以正确地实现 1/2、1/4、1/8 等的数值运算。如果是正数，只需在最高位补 0 即可。

```sh
# 十进制 -4
11111100
# 逻辑右移 63
00111111
# 算术右移 -1
11111111
```

## 浮点数

> 当前主流处理器一般都能支持 32 位的单精度浮点数与 64 位的双精度浮点数的表示和计算，并且能遵循 IEEE754-1985 工业标准。现在此标准最新的版本是 2008，其中增加了对 16 位半精度浮点数以及 128 位四精度浮点数的描述。C 语言标准引入了一个浮点模型，可用来表达任意精度的浮点数，尽管当前主流 C 语言编译器尚未很好地支持半精度浮点数与四精度浮点数的表示和计算。<br>

IEEE754-1985 对规格化单精度浮点数的格式如下定义：
1. 1 位符号位，一般是最高位（31 位），表示正负号。0 表示正数，1 表示负数。
2. 8 位指数位，又称阶码，位于 23 到 30 位。
3. 23 位尾数，位于 0 到 22 位。

# 字符与编码集

## ASCII

- 使用 7 个 bits 就可以完全表示 ASCII 码
- 包含 95 个可打印字符
- 33 个不可打印字符(包括控制字符)

## Extended ASCII

- 常见数学运算符
- 带音标的欧洲字符
- 表格符等

## Unicode

- 兼容全球的字符集
- 统一码、万国码、单一码
- Unicode 定义了世界通用的符号集, UTF-* 实现了编码
- UTF-8 以字节为单位对 Unicode 进行编码

> UTF-8 字符编码格式与 UTF-16 字符编码格式都是当今非常常用的字符编码格式，它们都是根据 Unicode 这一计算机工业编码标准进行制定的。当今现代化计算机操作系统几乎都使用 UTF-8 作为其默认的系统字符编码格式，大部分集成开发环境也几乎默认使用 UTF-8 编码格式。Google 在 2008 年的报告中指出，在互联网上，UTF-8 编码已经成为 HTML 文件使用最多的字符编码格式。<br>
> 在当今许多文本解析上，大多都以 UTF-16 编码来收集源文件以及文本上的字符，因为 UTF-16 编码格式的可变性小，而且大部分情况下 2 个字节即可表示一个常用字符，因此用 UTF-16 编码格式的字符串一来不怎么耗费内存，二来能方便地确定字符串的长度，从而可以方便地进行插入、修改、删除等操作。因此对于字符串的编辑而言，它比 UTF-8 更具优势。<br>

### UTF-8

> UTF-8 字符编码是一种变长的字符编码格式，可兼容之前已有的 ASCII 编码格式。它在 1993 年 1 月首次官方发布，当时 UTF-8 编码的字符长度最多可长达 6 个字节，也就是说当时一个 UTF-8 编码的字符可占用 1 到 6 个字节，比如如果一个字符是与 ASCII 码相兼容的，那么它就只占 1 个字节。到了 2003 年 11 月，UTF-8 做了一些改动然后再次发布，这是为了让 UTF-8 与 UTF-16 能完整兼容转换，对 UTF-8 的表达范围做了裁剪，将它的最大长度缩减到了 4 个字节，并且要求它必须满足 [RFC 3629](https://datatracker.ietf.org/doc/rfc3629/) 标准中的约束。现在我们用的 UTF-8 编码格式都是基于 2003 年发布的标准实现的。

> 我们上面探讨了 UTF-8 是一种变长编码格式，其长度在 1 个字节到 4 个字节之间，那么一个 UTF-8 编码的字符是如何表达的呢？一个 UTF-8 编码的字符由两部分构成，第一部分是用于标识该字符一共需要多少字节的前缀比特标志。这个前缀比特要看第一个字节的高 5 位，如果是 0，表示当前字符由 1 个字节构成；如果是 3，表示当前字符由 2 个字节构成；如果是 7 表示当前字符由 3 个字节构成；如果是 15，则表示当前字符由 4 个字节构成。其实这也就意味着通过观察第一个字节的前导 1 的个数即可判断出当前字符由多少个字节构成。第二部分则是该字符的码点（codepoint）。所谓码点就是用于表示一个特定字符具有实际意义的编码值，该编码值是由 Unicode 组织制定的。比如对于一个兼容 ASCII 码的 UTF-8 编码字符 A 来说，其完整的二进制编码为 01000001。可见其前导 1 的个数为 0，说明它就由 1 个字节构成。然后它的码点即为 100 0001，即十六进制的 0x41。

字节个数|码点比特个数|起始码点|最后码点|字节 1|字节 2|字节 3|字节 4
:---:|:---:|:---|:---|:---|:---|:---|:---
1|7|0x0000|0x007F|0xxxxxxx|N/A|N/A|N/A
2|11|0x0080|0x07FF|110xxxxx|10xxxxxx|N/A|N/A
3|16|0x0800|0xFFFF|1110xxxx|10xxxxxx|10xxxxxx|N/A
4|21|0x10000|0x10FFFF|11110xxx|10xxxxxx|10xxxxxx|10xxxxxx

上表中 x 符号表示码点的一个比特。我们看到，为了与 ASCII 码完全兼容，Unicdoe 在制定 UTF-8 编码时，将没有任何前导 1 的字节表示为单字节 UTF-8 编码，然后从由 2 个字节构成的 UTF-8 编码开始，有多少个前导 1 就表明当前字符占用多少个字节。然后，对于一个字符的 UTF-8 编码，后续字节的编码都以 10 开头，这么做的好处是可用于校验当前字符编码的正确性，也可以避免解析到用于表示 UTF-8 字符串的结束符 `\0` 字符，因为它的编码值为 0，而有了前导 10 比特，则当前字节的最小值为 0x80，所以不可能会出现 0 的情况。这种特性也称为自同步（self-synchronizing）特性，它可在遍历 UTF-8 编码字符串的时候方便校验当前字符编码的正确性。

我们下面举一个具体例子来说明一个字符的 UTF-8 编码是如何构成的，我们这里用欧元符号 `€` 进行举例说明。在 Unicode 标准中，欧元符号 `€` 的码点为 `0x20AC`。那么可以根据以下步骤来构造出其 UTF-8 编码。

1. 由于 0x20AC 这个码点坐落于 0x0800 到 0xFFFF 之间，因此它最终的 UTF-8 编码应该由 3 个字节构成。
2. 我们将 0x20AC 用二进制数来表示，为 0010 0000 1010 1100。随后我们将这些比特插入到相应字节中。
3. 字节 1 具有 4 位固定前导比特 1110，而低 4 位用于存放码点的比特，因此字节 1 正好可以将码点的高 4 位放进去，那么得到 1110 0010。
4. 字节 2 具有 2 个固定的前导比特 10，可存放 6 位码点比特，因此把后续 6 位码点的二进制比特插入进去得到 1000 0010。
5. 字节 3 具有两个固定的前导比特 10，可存放 6 位码点比特，因此我们可以将剩余的 6 位码点二进制比特插入进去得到 1010 1100。

这样，我们整理得到欧元符号 `€` 的 UTF-8 编码的二进制表示为：1110 0010 1000 0010 10101100。用十六进制表达则是 0xE282AC。

### UTF-16

> 在 20 世纪 80 年代，人们开发出了双字节字符编码格式，那时就将它称为“Unicode”。随后，随着各种语言符号的加入，人们很快发现单单用双字节来表示一个字符远远不够，而此时已经有许多开发商基于这种双字节字符编码做了许多大型项目。比如 Java 一开始就是基于这种双字节编码的字符格式的，所以它能够支持使用汉字或其他文字来定义某个标识符，而不仅仅用 ASCII 码字符。到了 1996 年，Unicode 标准开发了 2.0 版本，将原先的双字节字符编码改造为变长的字符编码格式，称为 UTF-16 字符编码，而之前的“Unicode”则改称为“UCS-2”编码格式，其中 UCS 表示通用字符集。

> 因此，UTF-16 也是变长的 Unicode 字符编码格式，不过它与 UTF-8 不同的是，它只有两种长度，一种是占用 2 字节的编码格式，这种编码格式与 UCS-2 完全兼容；还有一种就是 4 字节编码格式。UTF-16 也有“码点”这个概念，并且一个字符的码点与 UTF-8 编码中的码点值都是一样的，因为这些都是由 Unicode 组织来制定的。

> UTF-16 编码根据字符对应的码点值，由三种不同区间范围而做出了不同的定义。

1. 从 0x0000 到 0x7FFF 以及从 0xE000 到 0xFFFF 两个区间范围

> 坐落在这两个区间范围内的 UTF-16 编码表示起来非常简单，就是当前字符所对应的码点值本身。这也是 UTF-16 与 UCS-2 相兼容的区间。Unicode 将坐落于这两个区间范围内的码点称为基本多语言平面（Basic Multilingual Plane），简称 BMP。比如，像美元 `$` 符号，它在 Unicode 中的码点与 ASCII 码中的一样，均为 0x24，所以它的 UTF-16 编码即为 0x0024。注意，尽管它用一个字节即可表示，但对于 UTF-16 来说，仍然需要占用 2 个字节。而欧元符号在 Unicode 中的码点为 0x20AC，所以它对应的 Unicode 编码值即为 0x20AC。

2. 从 0x010000 到 0x10FFFF 范围

> 我们看到，这个区间内的码点用两个字节已经无法表达，所以对于 UTF-16 编码而言就需要动用 4 个字节进行描述。这个范围区间又称为补充平面（Supplementary Plane），像 Emoji、一些历史脚本、非常少用的汉字等都坐落于此平面中。那么在此范围内的 UTF-16 编码应该如何计算呢？可以通过以下三步来获得：
> 1. 将码点值减去 0x010000，然后取差的低 20 位，使得结果留在 0x000000 到 0x0FFFFF。
> 2. 取步骤 1 所得结果的高 10 位，范围在 0x0000 到 0x03FF，将它加上 0xD800，得到第一个 16 位编码单元，这也称为高位替换，该值的范围在 0xD800 到 0xDBFF。
> 3. 对于由步骤 1 所得结果的低 10 位（范围也在 0x0000 到 0x03FF），将它加上 0xDC00，得到第二个 16 位编码单元，这也称为低位替换，该值的范围在 0xDC00 到 0xDFFF。

> 我们这里举两个例子来说明码点落在 0x010000 到 0x10FFFF 范围字符的 UTF-16 编码如何表示。首先我们看一个德撒律字母，该字母的码点定义为 0x10437。第一步，我们先将该码点值减去 0x10000，得到 0x0437。取该值的低 20 位，得到二进制值 0000 0000 0100 0011 0111。然后，取它高 10 位 —— 0000 0000 01，对应十六进制值为 0x0001，将它加上 0xD800 之后得到 0xD801，因此它的第一个 16 位编码单元的值就是 0xD801。最后，取 20 位值的低 10 位，得到 000011 0111，对应十六进制值为 0x0037，将它加上 0xDC00 得到 0xDC37，因此它的第二个 16 位编码单元的值就是 0xDC37。最后我们得到整个德撒律字母的 UTF-16 编码表示为：0xD801DC37。

> 第二个例子是古代汉字“碎”，它的码点定义为 0x24B62。第一步，我们先将该码点值减去 0x10000，得到 0x14B62。第二步取它的低 20 位，得到二进制值 0001 0100 10110110 0010。第三步，取前 10 位，得到 00 0101 0010，对应十六进制值为 0x0052，将它加上 0xD800 之后得到 0xD852，因此第一个 16 位编码单元的值就是 0xD852。第四步，取刚才所得 20 位值的低 10 位，得到 11 0110 0010，对应十六进制值为 0x0362，将它加上 0xDC00 得到 0xDF62，因此第二个 16 位编码单元的值就是 0xDF62。最终，该汉字的 UTF-16 编码表示为 0xD852 DF62。

3. 范围从 0xD800 到 0xDFFF 的区间

> Unicode 永久保留了这两个区间不允许定义任何有意义的字符码点。因为我们通过上面从 0x010000 到 0x10FFFF 范围的 UTF-16 编码表示法就已经知道这个区间用于 4 字节 UTF-16 编码的高位替换与低位替换区间，所以不能用来定义码点值，否则会在编码上产生歧义。

## 中文编码集

- GB2312<br>
    《信息交换用汉字编码字符集---基本集》<br>
    一共收录了 7445 个字符<br>
    包括 6763 个汉字和 682 个其它符号<bf>
    不支持国际 ISO 标准<br>
- GBK<br>
    《汉字内码扩展规范》<br>
    向下兼容 GB2312, 向上支持国际 ISO 标准<br>
    收录了 21003 个汉字, 支持全部中日韩汉字<br>

# 地址与字节对齐

> 当前大部分处理器在操作系统的应用层所访问到的逻辑地址，而部分嵌入式系统由于不含带存储器管理单元，因此可直接访问物理地址。在计算机中，所谓“地址”就是用来标识存储单元的一个编号，就好比我们住房的门牌号。没有门牌号，快递就没法发货；如果门牌号记错了，那么快递就会把货物送错地方。计算机中的地址也是一样，我们为了要访问存储器中特定单元的一个数据，那么我们首先要获悉该数据所在的地址，然后我们通过这个地址来访问它。访问存储器，我们也简称为“访存”（Memory Access）。访问地址，我们也简称为“寻址”（Addressing）。一般计算机架构中都会有地址总线和数据总线。CPU 先通过地址总线发送寻址信号，以指定所要访问存储器单元的地址。然后再通过数据总线向该地址读写数据，这样就完成了一次访存操作。这好比于快递送货，我们先打电话告诉快递通信地址，然后快递员把货送到该地址（写数据），或者去该地址拿货（读数据）送到别家。<br>
> 一般对于 32 位系统来说，处理器一次可访问 1 个（8 比特）字节、2 个字节或 4 个字节。当访问单个字节时，对 CPU 不做对齐限制；而当访问多个字节时，比如要访问 N 个字节，由于计算机总线设计等诸多因素，要求 CPU 所访问的起始地址满足 N 个字节的倍数来访问存储器。如果在访问存储器时没有按照特定要求做字节对齐，那么可能会引发访存性能问题，甚至直接导致寻址错误而引发异常（引发异常后通常会导致当前应用意外退出，在嵌入式系统中可能就直接死机或复位）。<br>
> 当访存不满足对齐要求时并不会引发总线异常，但是访问性能会降低很多。因为原本可一次通信的数据传输可能需要拆分为多次，并且前后还要保证数据的一致性，所以还可能会有锁步之类的操作。而像 Blackf in DSP 则会直接引发总线异常，导致整个系统的崩溃（如果不对此异常做处理的话）。另外，像 ARMv5 或更低版本的处理器，在对非对齐的存储器地址进行访问时，CPU 会先自动向下定位到对齐地址，然后通过向右循环移位的方式处理数据，这就使得传输数据并不是原本想一次传输的数据内容，也就是说写入的或读出的数据是失真的。<br>

# 大端与小端

现代计算机系统中含有两种存放数据的字节序：大端（Big-endian）和小端（Little-endian）。所谓大端字节序是指在读写一个大于 1 个字节的数据时，其数据的最高字节存放在起始地址单元处，数据的最低字节存放在最高地址单元处。所谓小端字节序是指在读写一个大于 1 个字节的数据时，其数据的最低字节存放在起始地址单元处，而数据的最高字节存放在最高地址单元处。比如，我们要在地址 0x00001000 处存放一个 0x04030201 的 32 位整数，其大端、小端存放情况:

```sh
大端字节序:  0x04    0x03    0x02    0x01
     地址:  0x1000  0x1001  0x1002  0x1003
小端字节序:  0x01    0x02    0x03    0x04
```

当前，通用桌面处理器以及智能移动设备的处理器一般都用小端字节序。通信设备中用大端字节序比较普遍。

# 资源

[了不起的 Unicode！](https://mp.weixin.qq.com/s/GKpQkoujft2s6j95yZqzsA)<br>
